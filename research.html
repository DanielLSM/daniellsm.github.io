---
layout: layout
title: "Research Overview"
---

<center>  
  Here's my <a href="https://scholar.google.com/citations?user=sfM-khoAAAAJ&hl=en">Google Scholar</a>. Here's <a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html">how one should do research</a> according to <a href="https://en.wikipedia.org/wiki/Richard_Hamming">Richard Hamming</a>. <br>
  Jump to <a href='#publications'> selected publications</a> or <a href='#workshops'>workshops</a>.
</center>

<div class="divider"></div>

<h2 id='publications' class="page-heading"> Selected Publications</h1>

<div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/LLMpolite.png">
    <table>
      <tr>          
        <td><a href="https://arxiv.org/abs/2402.15420">arXiv</a></td> 
        <td><a href="https://dl.acm.org/doi/10.1145/3610977.3634970">HRI conference</a></td>
      </tr>
    </table>
  </div>

  <div class="six columns">

    <b> PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning </b>
    <p> <b>Simon Holk*</b>, Daniel Marta*, and Iolanda Leite <br />
    HRI'24 </p>
  <p> This paper aims to utilize the zero-shot capabilities of LLM in order to increase the granularity
    of preference-based feedback by extending my previous work. By letting the user give optional auxiliary
    textual descriptions of their preference we ensure that the reward function align with what the human
    actually wants as well as improve the sample-efficiency by avoiding over-sampling.
  </p>
  </div>
</div>

<div class="divider"></div>


<div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/POLITE1.png">
    <table>
      <tr>          
        <td><a href="https://ieeexplore.ieee.org/document/10610505">IEEE</a></td>
      </tr>
    </table>
  </div>

  <div class="six columns">

    <b> POLITE: Preferences Combined with Highlights in Reinforcement Learning </b>
    <p> <b>Simon Holk</b>, Daniel Marta, and Iolanda Leite <br />
    ICRA'24 - <b>Nominated for best HRI paper, best student paper, and best conference paper.</b></p>
  <p> This paper aims to improve the granularity of preference-based feedback by adding temporal trajectory segmentation
    to highlight positive and negative parts. This helps avoid the uniform assignment of responsibility across the whole trajectory
    while it might just be some part that was actually preferred. The highlights are then optimized as an auxiliary task ensuring
    a improved shared representation.
  </p>
  </div>
</div>

<div class="divider"></div>

<div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/Sequelframework.png">
    <table>
      <tr>          
        <td><a href="https://ieeexplore.ieee.org/document/10610534">IEEE</a></td>
      </tr>
    </table>
  </div>

  <div class="six columns">

    <b> SEQUEL: Semi-Supervised Preference-based RL with Query Synthesis via Latent Interpolation</b>
    <p> Daniel Marta*, <b>Simon Holk*</b>, Christian Pek, Jana Tumova, and Iolanda Leite <br />
    ICRA'24 </p>
  <p> By utilizing semi-supervised learning we can improve the sample-efficiency by augmenting the preferences provided by humans.
    We train a VAE to learn a latent representation and thereafter synthesise new queries from existing ones by interpolating the
    trajectory pairs. The idea is that if a user prefers traj A over traj B, even if they are a bit closer (like 10%) they would
    still prefer it. This helps the feedback generalize better.
  </p>
  </div>
</div>

<div class="divider"></div>

<div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/iros23.png">
    <table>
      <tr>          
        <td><a href="https://www.diva-portal.org/smash/get/diva2:1787953/FULLTEXT01.pdf">DiVA</a></td>
      </tr>
    </table>
  </div>

  <div class="six columns">

    <b> VARIQuery: VAE Segment-based Active Learning for Query Selection in Preference-based Reinforcement Learning </b>
    <p> <b>Daniel Marta*</b>, Simon Holk*, Christian Pek, Jana Tumova, and Iolanda Leite <br />
    IROS'23 </p>
  <p> This paper addresses the
    often-overlooked aspect of query selection, which is closely
    related to active learning (AL). We propose a novel query
    selection approach that leverages variational autoencoder (VAE)
    representations of state sequences. In this manner, we formulate
    queries that are diverse in nature while simultaneously taking
    into account reward model estimations.
  </p>
  </div>
</div>

<div class="divider"></div>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/icra23.png">
      <table>
        <tr>          
          <td><a href="https://www.diva-portal.org/smash/get/diva2:1744884/FULLTEXT01.pdf">DiVA</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Aligning Human Preferences with Baseline Objectives in Reinforcement Learning </b>
      <p> <b>Daniel Marta</b>, Simon Holk, Christian Pek, Jana Tumova, and Iolanda Leite <br />
      ICRA'23 </p>
    <p> By considering baseline objectives to be designed
      beforehand, we are able to narrow down the policy space, solely
      requesting human attention when their input matters the most.
      To allow for control over the optimization of different objectives,
      our approach contemplates a multi-objective setting. We achieve
      human-compliant policies by sequentially training an optimal
      policy from a baseline specification and collecting queries on
      pairs of trajectories. 
    </p>
    </div>
  </div>

  <div class="divider"></div>

    <div class="row">
      <div class="six columns">
        <img style="margin-top:0em" src="/images/research/ral21.png">
        <table>
          <tr>          
            <td><a href="https://www.diva-portal.org/smash/get/diva2:1612302/FULLTEXT01.pdf">DiVA</a></td>
          </tr>
        </table>
      </div>
  
      <div class="six columns">
  
        <b> Human-feedback shield synthesis for perceived safety in deep reinforcement learning </b>
        <p> <b>Daniel Marta</b>, Christian Pek, Gaspar I. Melsion, Jana Tumova, and Iolanda Leite  <br />
        RAL'21 </p>
      <p> To obtain policies that are perceived
        as safe, we propose a shield synthesis framework with two
        distinct loops: (1) an inner loop that trains policies with a set
        of actions that is constrained by shields whose conservativeness
        is parameterized, and (2) an outer loop that presents example
        rollouts of the policy to humans and collects their feedback
        to update the parameters of the shields in the inner loop.
      </p>
      </div>
    </div>
  
    <div class="divider"></div>


<h2 id='workshops' class="page-heading"> Workshops</h1>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/arxiv1.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2311.08206">ArXiv</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Human-Centric Autonomous Systems With LLMs for User Command Reasoning </b>
      <p> Yi Yang, Qingwen Zhang, Ci Li, <b>Daniel Marta</b>, Nazre Batool, John Folkesson <br />
    </p>
    <p> To ensure that the autonomous system
      meets the user’s intent, it is essential to accurately discern
      and interpret user commands, especially in complex or
      emergency situations. To this end, we propose to leverage
      the reasoning capabilities of Large Language Models
      (LLMs) to infer system requirements from in-cabin users’
      commands. Through a series of experiments that include
      different LLM models and prompt designs, we explore
      the few-shot multivariate binary classification accuracy
      of system requirements from natural language textual
      commands.
    </p>
    </div>
  </div>

</div>
